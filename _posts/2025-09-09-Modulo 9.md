---
layout: post
title: "üåç M√≥dulo 9 ‚Äì An√°lisis de Movimientos Migratorios con Spark"
date: 2025-09-09 07:00:00 -0300
---



Este proyecto corresponde al **M√≥dulo 9 del Bootcamp de Ciencia de Datos**.  
El objetivo fue aplicar conceptos de **Big Data con Apache Spark y PySpark** para analizar patrones de migraci√≥n humana, utilizando RDDs, DataFrames, Spark SQL y MLlib para predicci√≥n de flujos migratorios.

---

## üìò Enunciado

**EVALUACI√ìN FINAL: AN√ÅLISIS DE MOVIMIENTOS MIGRATORIOS CON SPARK**  

Eres parte de un equipo de analistas de datos encargado de estudiar las tendencias de migraci√≥n humana en el siglo XXI utilizando Big Data. Para ello, trabajar√°s con un conjunto de datos que contiene informaci√≥n sobre migraciones entre distintos pa√≠ses, sus causas y el impacto socioecon√≥mico en las regiones de origen y destino.  

**Objetivos de la actividad:**
1. Aplicar conceptos de Big Data utilizando Apache Spark y PySpark.  
2. Explorar y transformar datos con RDDs y DataFrames.  
3. Realizar consultas con Spark SQL.  
4. Implementar modelos de aprendizaje autom√°tico con MLlib.  

---

## üåê Repositorio
- [Ver en GitHub](https://github.com/LirBast/Portafolio/tree/portafolio/modulo_9)  

---

## üõ†Ô∏è Tecnolog√≠as usadas
- Python + Google Colab
 
- **Apache Spark / PySpark** (RDDs, DataFrames, Spark SQL, MLlib)  
- Machine Learning con **Logistic Regression** en MLlib  

---

## üìã Requerimientos implementados

1. **üìä Carga y exploraci√≥n de datos **  
   - Carga del dataset con informaci√≥n migratoria.  
   - Conversi√≥n a **RDD y DataFrame**.  
   - Exploraci√≥n: primeras filas, esquema y estad√≠sticas descriptivas.  

2. **‚öôÔ∏è Procesamiento de datos con RDDs y DataFrames **  
   - Transformaciones en RDDs: `filter`, `map`, `flatMap`.  
   - Acciones en RDDs: `take`, `count`, `collect`.  
   - Operaciones en DataFrames: filtrado, agregaciones y ordenamiento.  
   - Exportaci√≥n de resultados en **formato Parquet**.  

3. **üóÇÔ∏è Consultas con Spark SQL **  
   - Registro del DataFrame como **tabla temporal**.  
   - Consultas de los principales pa√≠ses de origen y destino.  
   - An√°lisis de las principales **razones de migraci√≥n por regi√≥n**.  

4. **ü§ñ Aplicaci√≥n de MLlib para predicci√≥n **  
   - Conversi√≥n de datos en vectores de caracter√≠sticas (`VectorAssembler`).  
   - Entrenamiento de un modelo de **Regresi√≥n Log√≠stica**.  
   - Evaluaci√≥n de precisi√≥n del modelo.  

---

## üìä Resultados principales
- RDDs permitieron un manejo flexible de transformaciones (ej. filtrar migraciones econ√≥micas).  
- DataFrames y Spark SQL facilitaron consultas complejas, como identificar los principales destinos o las razones m√°s comunes de migraci√≥n.  
- Se guardaron resultados de agregaciones en formato Parquet para optimizar el almacenamiento.  
- El modelo de **regresi√≥n log√≠stica** en MLlib logr√≥ ejecutarse, pero debido al tama√±o muy reducido del dataset (solo 5 registros), la precisi√≥n fue **0**, ya que el conjunto de prueba qued√≥ con un solo dato.  

---

## üìù Reflexi√≥n Final

Este m√≥dulo permiti√≥ integrar el uso de **Big Data con Apache Spark**, explorando c√≥mo trabajar con RDDs, DataFrames y Spark SQL en un flujo completo de an√°lisis. Las transformaciones con RDDs fueron √∫tiles para entender la estructura de los datos en bajo nivel, mientras que los DataFrames y consultas SQL ofrecieron mayor expresividad y eficiencia para agregaciones y an√°lisis exploratorio. La exportaci√≥n en Parquet reflej√≥ la importancia de usar formatos optimizados en entornos Big Data.  

En cuanto al modelado predictivo, la regresi√≥n log√≠stica en MLlib mostr√≥ el procedimiento de preparaci√≥n de datos, entrenamiento y evaluaci√≥n. Sin embargo, el tama√±o extremadamente reducido del dataset no permiti√≥ obtener m√©tricas representativas, lo que evidencia la necesidad de **grandes vol√∫menes de datos** para que Spark muestre todo su potencial.  

En s√≠ntesis, este m√≥dulo fue un acercamiento valioso a la **anal√≠tica de datos a gran escala**, reforzando la importancia del contexto Big Data y el uso de Spark como herramienta central para el procesamiento y an√°lisis de informaci√≥n compleja.  

---

‚úçÔ∏è *Autor: Liroy Cataldo*

